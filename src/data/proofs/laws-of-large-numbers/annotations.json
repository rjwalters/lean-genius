[
  {
    "id": "wlln-chebyshev",
    "proofId": "laws-of-large-numbers",
    "range": {
      "startLine": 178,
      "endLine": 195
    },
    "type": "theorem",
    "title": "Weak Law of Large Numbers",
    "content": "For i.i.d. random variables with finite variance, the sample mean converges in probability to the population mean. The proof uses Chebyshev's inequality: P(|X̄ₙ - μ| ≥ ε) ≤ σ²/(nε²) → 0.",
    "mathContext": "This is Wiedijk's Theorem #59 (partial). The variance of the sample mean is σ²/n, which goes to 0, making large deviations from μ increasingly unlikely.",
    "significance": "critical",
    "prerequisites": ["variance", "chebyshev-inequality", "convergence-probability"],
    "relatedConcepts": ["central-limit-theorem", "strong-law"]
  },
  {
    "id": "slln-mathlib",
    "proofId": "laws-of-large-numbers",
    "range": {
      "startLine": 247,
      "endLine": 267
    },
    "type": "theorem",
    "title": "Strong Law of Large Numbers",
    "content": "Mathlib's strong_law_ae proves that for i.i.d. integrable random variables, the sample mean converges almost surely to the expectation. This only requires pairwise independence.",
    "mathContext": "Almost sure convergence means the set of outcomes where convergence fails has probability zero. This is strictly stronger than convergence in probability.",
    "significance": "critical",
    "prerequisites": ["almost-sure-convergence", "integrability"],
    "relatedConcepts": ["wlln", "ergodic-theorem"]
  },
  {
    "id": "variance-sample-mean",
    "proofId": "laws-of-large-numbers",
    "range": {
      "startLine": 165,
      "endLine": 175
    },
    "type": "lemma",
    "title": "Variance of Sample Mean",
    "content": "For i.i.d. random variables with variance σ², the variance of the sample mean is σ²/n. This follows from Var(ΣXᵢ/n) = Var(ΣXᵢ)/n² = nσ²/n² = σ²/n.",
    "mathContext": "Independence is crucial here: Var(X+Y) = Var(X) + Var(Y) only holds for independent variables.",
    "significance": "key",
    "prerequisites": ["variance", "independence"],
    "relatedConcepts": ["chebyshev-inequality"]
  },
  {
    "id": "ae-implies-prob",
    "proofId": "laws-of-large-numbers",
    "range": {
      "startLine": 290,
      "endLine": 305
    },
    "type": "theorem",
    "title": "Almost Sure Implies In Probability",
    "content": "Almost sure convergence implies convergence in probability. If Xₙ → X a.s., then for any ε > 0, P(|Xₙ - X| > ε) → 0.",
    "mathContext": "The converse is false: convergence in probability does not imply almost sure convergence. A counterexample involves random variables that 'flicker' back and forth.",
    "significance": "key",
    "prerequisites": ["almost-sure-convergence", "convergence-probability"],
    "relatedConcepts": ["modes-of-convergence"]
  },
  {
    "id": "chebyshev-ineq",
    "proofId": "laws-of-large-numbers",
    "range": {
      "startLine": 143,
      "endLine": 162
    },
    "type": "theorem",
    "title": "Chebyshev's Inequality",
    "content": "P(|X - μ| ≥ ε) ≤ Var(X)/ε². This follows from Markov's inequality applied to (X - μ)².",
    "mathContext": "Chebyshev's inequality gives a universal bound on tail probabilities using only the variance. It's tight for two-point distributions.",
    "significance": "key",
    "prerequisites": ["variance", "markov-inequality"],
    "relatedConcepts": ["concentration-inequalities", "wlln"]
  }
]
