[
  {
    "id": "ann-clt-intro",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 1, "endLine": 14 },
    "type": "concept",
    "title": "Dual Perspectives on CLT",
    "content": "This proof presents the Central Limit Theorem through two complementary lenses. The **classical approach** uses characteristic functions (Fourier transforms) to show convergence. The **topological approach** reveals why the Gaussian emerges: it is a fixed-point attractor in the dynamics of averaging.",
    "significance": "critical",
    "relatedConcepts": ["characteristic functions", "fixed point theory", "renormalization group"]
  },
  {
    "id": "ann-clt-imports",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 16, "endLine": 25 },
    "type": "concept",
    "title": "Mathlib Dependencies",
    "content": "We import from multiple Mathlib domains: **Probability.Distributions.Gaussian** for the Gaussian measure, **Fourier.FourierTransform** for characteristic functions, and **Topology.MetricSpace** for the geometric interpretation. This interdisciplinary foundation reflects the theorem's broad reach.",
    "significance": "supporting",
    "relatedConcepts": ["Mathlib", "measure theory", "Fourier analysis"]
  },
  {
    "id": "ann-clt-charfun-def",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 38, "endLine": 41 },
    "type": "definition",
    "title": "Characteristic Functions",
    "content": "The **characteristic function** $\\varphi_X(t) = \\mathbb{E}[e^{itX}]$ is the Fourier transform of a probability distribution. It uniquely determines the distribution and has a crucial property: for independent $X, Y$, we have $\\varphi_{X+Y} = \\varphi_X \\cdot \\varphi_Y$. This converts convolution (sums of random variables) into multiplication.",
    "mathContext": "Characteristic functions always exist (bounded exponential integrand) and are uniformly continuous. They encode all moments: $\\varphi^{(k)}(0) = i^k \\mathbb{E}[X^k]$ when moments exist.",
    "significance": "critical",
    "relatedConcepts": ["Fourier transform", "moment generating function", "convolution"]
  },
  {
    "id": "ann-clt-charfun-zero",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 43, "endLine": 48 },
    "type": "lemma",
    "title": "Characteristic Function at Zero",
    "content": "At $t = 0$, the characteristic function equals 1 for any probability measure: $\\varphi(0) = \\mathbb{E}[e^{i \\cdot 0 \\cdot X}] = \\mathbb{E}[1] = 1$. This normalization anchors the Fourier representation.",
    "mathContext": "This simple fact is essential: it means all characteristic functions pass through the same point, allowing us to compare different distributions on a common scale.",
    "significance": "supporting",
    "relatedConcepts": ["probability normalization", "Fourier analysis"]
  },
  {
    "id": "ann-clt-gaussian-charfun",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 50, "endLine": 57 },
    "type": "theorem",
    "title": "Gaussian Characteristic Function",
    "content": "The standard Gaussian $N(0,1)$ has characteristic function $\\varphi(t) = e^{-t^2/2}$. This is **self-dual** under Fourier transform: the Gaussian is (essentially) its own Fourier transform! This remarkable property explains why $e^{-t^2/2}$ appears on 'both sides' of CLT.",
    "mathContext": "Computing: $\\mathbb{E}[e^{itX}] = \\int e^{itx} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} dx = e^{-t^2/2}$ by completing the square in the exponent. The Gaussian integral formula is crucial here.",
    "significance": "critical",
    "relatedConcepts": ["Gaussian integral", "self-duality", "Fourier transform"]
  },
  {
    "id": "ann-clt-taylor-expansion",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 68, "endLine": 72 },
    "type": "insight",
    "title": "Moments from Derivatives",
    "content": "Taking derivatives of the characteristic function at $t=0$ extracts moments: $\\varphi'(0) = i\\mu$ (mean), $\\varphi''(0) = i^2(\\mu^2 + \\sigma^2)$ (second moment). The Taylor expansion $\\varphi(t) = 1 + it\\mu - t^2\\sigma^2/2 + O(t^3)$ encodes the distribution's shape in its coefficients.",
    "significance": "key",
    "relatedConcepts": ["Taylor series", "moments", "cumulants"]
  },
  {
    "id": "ann-clt-taylor-theorem",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 74, "endLine": 84 },
    "type": "lemma",
    "title": "Taylor Expansion with Error Bound",
    "content": "For small $t$, the characteristic function is well-approximated by its quadratic Taylor polynomial. The error is $O(t^3)$, controlled by the third moment. This approximation is sharp enough to extract the limiting behavior.",
    "mathContext": "The third moment condition $\\mathbb{E}[|X|^3] < \\infty$ ensures the remainder is well-controlled. Without it, convergence may fail (leading to stable distributions instead).",
    "significance": "key",
    "prerequisites": ["ann-clt-charfun-def"],
    "relatedConcepts": ["Taylor's theorem", "error bounds", "third moment"]
  },
  {
    "id": "ann-clt-limit-setup",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 95, "endLine": 104 },
    "type": "concept",
    "title": "The Product-to-Exponential Limit",
    "content": "The classical limit $(1 + x/n)^n \\to e^x$ is the engine of CLT. For the normalized sum $S_n = (X_1 + \\cdots + X_n)/\\sqrt{n}$, we have $\\varphi_{S_n}(t) = [\\varphi_X(t/\\sqrt{n})]^n$. Taylor expansion gives $\\varphi_X(t/\\sqrt{n}) \\approx 1 - t^2/(2n)$, and $(1 - t^2/(2n))^n \\to e^{-t^2/2}$.",
    "significance": "critical",
    "relatedConcepts": ["exponential limit", "compound interest", "Euler's definition of e"]
  },
  {
    "id": "ann-clt-key-convergence",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 117, "endLine": 131 },
    "type": "theorem",
    "title": "Heart of CLT: Characteristic Function Convergence",
    "content": "**The Central Computation**: As $n \\to \\infty$, the characteristic function of $S_n$ converges pointwise to $e^{-t^2/2}$. This is the analytical core of CLT.\n\n**Topological insight**: We're watching the orbit of a distribution under the renormalization map $T_n(\\mu) = (1/\\sqrt{n}) \\cdot \\mu^{*n}$. The Gaussian is a fixed point, and all finite-variance distributions are in its basin of attraction.",
    "mathContext": "The third moment condition ensures uniform convergence on compact sets, which is needed for the next step (Lévy's theorem).",
    "significance": "critical",
    "prerequisites": ["ann-clt-taylor-theorem", "ann-clt-limit-setup"],
    "relatedConcepts": ["pointwise convergence", "basin of attraction", "fixed point"]
  },
  {
    "id": "ann-clt-levy-theorem",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 145, "endLine": 159 },
    "type": "theorem",
    "title": "Lévy Continuity Theorem",
    "content": "**The Bridge**: Pointwise convergence of characteristic functions to a continuous function implies weak convergence of probability measures. This connects the Fourier (analysis) world to the distributional (probability) world.\n\nLévy's theorem is the inverse of the continuity of the Fourier transform: if the images converge, so do the preimages.",
    "mathContext": "Continuity at $t=0$ is essential and automatic for characteristic functions (they equal 1 at 0). The theorem fails for discontinuous limits.",
    "significance": "critical",
    "relatedConcepts": ["weak convergence", "Fourier inversion", "tightness"]
  },
  {
    "id": "ann-clt-main-theorem",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 172, "endLine": 192 },
    "type": "theorem",
    "title": "The Central Limit Theorem",
    "content": "**Statement**: Let $X_1, X_2, \\ldots$ be i.i.d. with mean $\\mu$ and variance $\\sigma^2$. The normalized sum $Z_n = (\\sum X_i - n\\mu)/(\\sigma\\sqrt{n})$ converges in distribution to $N(0,1)$.\n\n**Classical interpretation**: Sums of random effects average toward the bell curve.\n\n**Topological interpretation**: The renormalization flow contracts all finite-variance distributions toward the Gaussian fixed point.",
    "mathContext": "The proof assembles: (1) Taylor expansion of characteristic function, (2) product-to-exponential limit, (3) Lévy continuity. Each piece is classical; together they prove the most useful theorem in probability.",
    "significance": "critical",
    "prerequisites": ["ann-clt-key-convergence", "ann-clt-levy-theorem"],
    "relatedConcepts": ["convergence in distribution", "limit theorem", "universal law"]
  },
  {
    "id": "ann-clt-topological-intro",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 200, "endLine": 215 },
    "type": "concept",
    "title": "The Topological Viewpoint",
    "content": "Now we shift perspective: instead of computing limits, we ask WHY the Gaussian emerges. The answer: **it is a fixed point attractor** of the renormalization group flow on the space of probability distributions.\n\nThis dynamical systems perspective reveals that CLT is not an accident of computation but a consequence of the structure of the convolution operation.",
    "significance": "critical",
    "relatedConcepts": ["dynamical systems", "fixed point theory", "renormalization group"]
  },
  {
    "id": "ann-clt-wasserstein",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 217, "endLine": 235 },
    "type": "concept",
    "title": "The Geometry of Probability Distributions",
    "content": "The space $\\mathcal{P}_2(\\mathbb{R})$ of probability measures with finite second moment forms a metric space under the **Wasserstein-2 distance**: $W_2(\\mu, \\nu)^2 = \\inf \\int |x-y|^2 \\, d\\gamma(x,y)$ over all couplings $\\gamma$.\n\nThis is the natural geometry for CLT because:\n- Convolution is well-defined and continuous\n- Variance is a continuous functional\n- The Gaussian has finite distance to all distributions in $\\mathcal{P}_2$",
    "mathContext": "The Wasserstein metric captures the 'cost' of transporting one distribution to another. It makes $\\mathcal{P}_2$ a complete metric space (Villani's work on optimal transport).",
    "significance": "key",
    "relatedConcepts": ["optimal transport", "Wasserstein metric", "metric geometry"]
  },
  {
    "id": "ann-clt-renormalization-def",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 237, "endLine": 245 },
    "type": "definition",
    "title": "The Renormalization Map",
    "content": "Define $T_n(\\mu) = \\frac{1}{\\sqrt{n}} \\cdot \\mu^{*n}$, where $\\mu^{*n}$ is the $n$-fold convolution (distribution of sum of $n$ i.i.d. copies) and the factor $1/\\sqrt{n}$ rescales to preserve variance.\n\nThis map encapsulates 'averaging': take $n$ independent samples, add them, normalize. CLT asks: what happens as $n \\to \\infty$?",
    "mathContext": "In physics, this is a 'renormalization group' flow: we coarse-grain (average) and rescale. Fixed points are 'scale-invariant' distributions.",
    "significance": "key",
    "relatedConcepts": ["convolution", "rescaling", "renormalization group"]
  },
  {
    "id": "ann-clt-fixed-point",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 247, "endLine": 260 },
    "type": "theorem",
    "title": "Gaussian is a Fixed Point",
    "content": "**Key Insight**: The Gaussian is unchanged by the renormalization map! If $X_1, \\ldots, X_n \\sim N(0,1)$ are i.i.d., then $(X_1 + \\cdots + X_n)/\\sqrt{n} \\sim N(0,1)$.\n\nThis self-reproducing property is the **topological essence of CLT**: the Gaussian is the unique stable distribution under convolution-rescaling at the $\\sqrt{n}$ scale.",
    "mathContext": "The calculation: $X_i \\sim N(0,1)$ implies $\\sum X_i \\sim N(0,n)$, so $(\\sum X_i)/\\sqrt{n} \\sim N(0,1)$. Variance rescales as $n/n = 1$.",
    "significance": "critical",
    "relatedConcepts": ["fixed point", "scale invariance", "self-similarity"]
  },
  {
    "id": "ann-clt-attractor",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 262, "endLine": 280 },
    "type": "theorem",
    "title": "Gaussian as Universal Attractor",
    "content": "**CLT Restated**: Every distribution with finite variance is in the basin of attraction of the Gaussian under the renormalization flow. Formally, $T_n(\\mu) \\to \\mathcal{N}$ in the Wasserstein metric as $n \\to \\infty$.\n\nThis is the dynamical systems view: we have a discrete dynamical system on $\\mathcal{P}_2$ with the Gaussian as a globally attracting fixed point (for finite-variance starting points).",
    "mathContext": "The 'domain of attraction' is exactly the set of distributions with finite variance. Infinite variance leads to stable distributions with $\\alpha < 2$ (Lévy flights).",
    "significance": "critical",
    "prerequisites": ["ann-clt-fixed-point"],
    "relatedConcepts": ["basin of attraction", "global attractor", "domain of attraction"]
  },
  {
    "id": "ann-clt-why-gaussian-entropy",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 301, "endLine": 311 },
    "type": "insight",
    "title": "Maximum Entropy Characterization",
    "content": "The Gaussian **maximizes entropy** among all distributions with fixed variance. This information-theoretic characterization explains why thermal fluctuations (maximum disorder given constraints) are Gaussian.\n\n$H(\\mu) \\leq H(\\mathcal{N})$ for all $\\mu$ with $\\text{Var}(\\mu) = \\text{Var}(\\mathcal{N})$, with equality iff $\\mu = \\mathcal{N}$.",
    "mathContext": "Maximum entropy = maximum uncertainty = the 'most random' distribution consistent with known constraints (Jaynes' principle). This connects thermodynamics to probability.",
    "significance": "key",
    "relatedConcepts": ["Shannon entropy", "maximum entropy principle", "thermodynamics"]
  },
  {
    "id": "ann-clt-self-duality",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 313, "endLine": 320 },
    "type": "theorem",
    "title": "Fourier Self-Duality",
    "content": "The Gaussian is its own Fourier transform (up to scaling). This explains why the same function $e^{-t^2/2}$ appears as both the characteristic function AND the probability density (after appropriate normalization).\n\nSelf-duality under Fourier transform is a unique property that singles out the Gaussian among all probability distributions.",
    "mathContext": "More precisely: if $f(x) = e^{-x^2/2}/\\sqrt{2\\pi}$, then $\\hat{f}(\\xi) = e^{-\\xi^2/2}$. The Hermite functions (Gaussian times polynomials) form an orthonormal basis of eigenfunctions for the Fourier transform.",
    "significance": "key",
    "relatedConcepts": ["Fourier transform", "eigenfunctions", "Hermite polynomials"]
  },
  {
    "id": "ann-clt-berry-esseen",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 335, "endLine": 350 },
    "type": "theorem",
    "title": "Berry-Esseen: Rate of Convergence",
    "content": "**Quantitative CLT**: The Kolmogorov distance between $F_{Z_n}$ and $\\Phi$ (standard normal CDF) is $O(\\rho/\\sqrt{n})$, where $\\rho = \\mathbb{E}[|X|^3]$ is the third absolute moment.\n\n$\\sup_x |F_{Z_n}(x) - \\Phi(x)| \\leq \\frac{C \\rho}{\\sqrt{n}}$ with $C \\approx 0.4748$.\n\nThis tells us HOW FAST the convergence occurs, not just that it happens.",
    "mathContext": "The $1/\\sqrt{n}$ rate is optimal: there exist distributions achieving this rate. The constant $C$ has been refined by many authors.",
    "significance": "key",
    "relatedConcepts": ["rate of convergence", "Kolmogorov distance", "third moment"]
  },
  {
    "id": "ann-clt-stable",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 352, "endLine": 362 },
    "type": "insight",
    "title": "Stable Distributions: Beyond Gaussian",
    "content": "When variance is infinite, the Gaussian is no longer the attractor. Instead, **$\\alpha$-stable distributions** (Lévy distributions) emerge as limits. The Gaussian corresponds to $\\alpha = 2$; the Cauchy to $\\alpha = 1$.\n\nHeavy-tailed phenomena (earthquakes, financial crashes, network traffic) often follow stable laws with $\\alpha < 2$, exhibiting infinite variance and 'Lévy flights.'",
    "mathContext": "A distribution is $\\alpha$-stable if it is closed under convolution and appropriate rescaling: $X_1 + X_2 \\stackrel{d}{=} cX$ for some $c$. Only for $\\alpha = 2$ is variance finite.",
    "significance": "key",
    "relatedConcepts": ["stable distributions", "Lévy flights", "heavy tails", "power laws"]
  },
  {
    "id": "ann-clt-conclusion",
    "proofId": "central-limit-theorem",
    "range": { "startLine": 370, "endLine": 390 },
    "type": "concept",
    "title": "Synthesis: Why the Gaussian is Universal",
    "content": "The Central Limit Theorem reveals the Gaussian as a **mathematical necessity**, not an accident:\n\n1. **Dynamically**: It is the unique fixed-point attractor of the averaging operation\n2. **Information-theoretically**: It maximizes entropy at fixed variance\n3. **Analytically**: It is self-dual under Fourier transform\n4. **Probabilistically**: It is the only finite-variance stable distribution\n\nWherever we see many small independent effects being summed—measurement errors, thermal fluctuations, stock prices, IQ scores—the Gaussian emerges because averaging dynamics demand it.",
    "significance": "critical",
    "relatedConcepts": ["universality", "emergence", "mathematical inevitability"]
  }
]
